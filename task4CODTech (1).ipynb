{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"The impact of artificial intelligence on healthcare\"\n",
        "output = generator(prompt, max_length=150, num_return_sequences=1)\n",
        "\n",
        "print(\"âœ… GPT-2 Generated Paragraph:\\n\")\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M53h_HJ0Zg8q",
        "outputId": "3ee18d3d-cedb-4f5b-8d1c-5d56a74b652a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPT-2 Generated Paragraph:\n",
            "\n",
            "The impact of artificial intelligence on healthcare is certainly massive. But that will only ever change if we believe in something called AI â€“ that is, the ability to control individuals who are smarter than us?\n",
            "\n",
            "In the US, this argument involves an unmitigated moral failure to address the growing potential that AI will replace humans. Despite our best efforts, such a philosophy exists only in theory: this is largely a fantasy, and it exists to serve its theoretical purposes, and to serve the interests of the corporate capitalist class. We are doing our best to avoid the problem, but this is the case only by making an artificial society with many different types â€“ with many different demographics, to varying degrees â€“ that can be used to solve complex problems. As such\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# ğŸ“˜ Longer sample paragraph for better learning\n",
        "text = \"\"\"\n",
        "Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling data-driven decisions.\n",
        "Its applications in healthcare, education, and transportation are reshaping the way we live and work.\n",
        "AI-powered tools are becoming increasingly integrated into daily life, driving innovation and new business models.\n",
        "\"\"\"\n",
        "\n",
        "# Character mapping\n",
        "chars = sorted(set(text))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(chars)\n",
        "hidden_size = 128\n",
        "seq_length = 40\n",
        "lr = 0.01\n",
        "\n",
        "# Data preparation\n",
        "def create_sequences(text, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = text[i:i+seq_length]\n",
        "        target = text[i+1:i+seq_length+1]\n",
        "        X.append([char_to_ix[ch] for ch in seq])\n",
        "        y.append([char_to_ix[ch] for ch in target])\n",
        "    return X, y\n",
        "\n",
        "X, y = create_sequences(text, seq_length)\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMTextGen(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMTextGen, self).__init__()\n",
        "        self.embed = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "model = LSTMTextGen(input_size, hidden_size)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training the model\n",
        "print(\"\\nâ³ Training LSTM...\")\n",
        "for epoch in range(100):\n",
        "    for i in range(len(X)):\n",
        "        input_seq = X[i].unsqueeze(1)\n",
        "        target_seq = y[i].unsqueeze(1)\n",
        "\n",
        "        hidden = model.init_hidden()\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_seq, hidden)\n",
        "        loss = loss_fn(output.view(-1, input_size), target_seq.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_str, length=500, temperature=1.0):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden()\n",
        "    input_seq = torch.tensor([[char_to_ix[ch] for ch in start_str]]).T\n",
        "    result = start_str\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        last_char_logits = output[-1]\n",
        "        probs = F.softmax(last_char_logits, dim=1).data.squeeze().pow(1 / temperature)\n",
        "        probs = probs / probs.sum()\n",
        "        char_ind = torch.multinomial(probs, 1)[0]\n",
        "        char = ix_to_char[char_ind.item()]\n",
        "        result += char\n",
        "        input_seq = torch.tensor([[char_ind]])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Generate paragraph using trained LSTM\n",
        "print(\"\\nâœ… LSTM Generated Paragraph:\\n\")\n",
        "print(generate_text(model, \"Artificial\", length=1000, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOrPUquRY72K",
        "outputId": "2790207e-4005-4423-89bb-c39f77a101da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â³ Training LSTM...\n",
            "Epoch 0, Loss: 0.4457\n",
            "Epoch 10, Loss: 0.0648\n",
            "Epoch 20, Loss: 0.0287\n",
            "Epoch 30, Loss: 0.0302\n",
            "Epoch 40, Loss: 0.0169\n",
            "Epoch 50, Loss: 0.0367\n",
            "Epoch 60, Loss: 0.0329\n",
            "Epoch 70, Loss: 0.0495\n",
            "Epoch 80, Loss: 0.0092\n",
            "Epoch 90, Loss: 0.0546\n",
            "\n",
            "âœ… LSTM Generated Paragraph:\n",
            "\n",
            "Artificial intellligence is transportation and new business modells are becoming increasinablintransportation and new business models.\n",
            "AI-powered tools are becomincreasin healthcare becoming increasinablthcare becoming increasinablinttrated into daily life, driving innovation and new business models.\n",
            "AI-powered toools are becomincy, and work.\n",
            "AI-powered tools are becoming increasinabling data-driven decision,shapingly integly integrated into daily life, driving innovation and new business models.\n",
            "AI-powered toools areforming increasinabling data-driven decisions.\n",
            "Its is is is integrated into daily life, driving innovation and new business modells.\n",
            "AI-powered toools abusfoming increasin healthcare becoming increasinablinttrated into daily life, driving innovation and new business modells.\n",
            "AI-powered tools are becoming increasin healthcare becoming increasinabling data-driven decisions.\n",
            "Its is is integrated into daily life, driving innovation and new business models.\n",
            "AI-powered tools are becoming \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzmQYcKhxdgZ",
        "outputId": "14953af5-ead1-4d71-d989-eae55b7c6768"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's bring in the libraries we need for text generation and the UI\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Here's the sample text we'll use to train the model\n",
        "text = \"\"\"\n",
        "Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling data-driven decisions.\n",
        "Its applications in healthcare, education, and transportation are reshaping the way we live and work.\n",
        "AI-powered tools are becoming increasingly integrated into daily life, driving innovation and new business models.\n",
        "\"\"\"\n",
        "\n",
        "# Let's map characters to indices for encoding\n",
        "chars = sorted(set(text))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Set up hyperparameters for the model\n",
        "input_size = len(chars)\n",
        "hidden_size = 128\n",
        "seq_length = 40\n",
        "lr = 0.01\n",
        "\n",
        "# Function to create sequences for training\n",
        "def create_sequences(text, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = text[i:i+seq_length]\n",
        "        target = text[i+1:i+seq_length+1]\n",
        "        X.append([char_to_ix[ch] for ch in seq])\n",
        "        y.append([char_to_ix[ch] for ch in target])\n",
        "    return X, y\n",
        "\n",
        "# Prepare the training data\n",
        "X, y = create_sequences(text, seq_length)\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# Define the LSTM model for text generation\n",
        "class LSTMTextGen(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMTextGen, self).__init__()\n",
        "        self.embed = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "# Initialize the model, loss function, and optimizer as global variables\n",
        "model = LSTMTextGen(input_size, hidden_size)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Train the model using global variables\n",
        "def train_model(epochs=100, progress=gr.Progress()):\n",
        "    try:\n",
        "        print(\"\\nâ³ Training LSTM...\")\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                input_seq = X[i].unsqueeze(1)\n",
        "                target_seq = y[i].unsqueeze(1)\n",
        "\n",
        "                hidden = model.init_hidden()\n",
        "                optimizer.zero_grad()\n",
        "                output, _ = model(input_seq, hidden)\n",
        "                loss = loss_fn(output.view(-1, input_size), target_seq.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Update progress every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                progress((epoch + 1) / epochs, desc=f\"Training... Epoch {epoch}/{epochs}\")\n",
        "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "        return \"Training completed successfully!\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during training: {str(e)}\"\n",
        "\n",
        "# Function to generate text based on a starting string using global model\n",
        "def generate_text(start_str):\n",
        "    try:\n",
        "        model.eval()\n",
        "        hidden = model.init_hidden()\n",
        "        # Prepare the input sequence from the starting string\n",
        "        input_seq = torch.tensor([[char_to_ix[ch] for ch in start_str if ch in char_to_ix]]).T\n",
        "        result = start_str\n",
        "\n",
        "        # Generate new characters one by one, with a fixed length of 500 characters and fixed temperature of 1.0\n",
        "        for _ in range(500):  # Fixed length\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            last_char_logits = output[-1]\n",
        "            probs = F.softmax(last_char_logits, dim=1).data.squeeze().pow(1 / 1.0)  # Fixed temperature\n",
        "            probs = probs / probs.sum()\n",
        "            char_ind = torch.multinomial(probs, 1)[0]\n",
        "            char = ix_to_char[char_ind.item()]\n",
        "            result += char\n",
        "            input_seq = torch.tensor([[char_ind]])\n",
        "\n",
        "        return result, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Error during text generation: {str(e)}\"\n",
        "\n",
        "# Custom CSS for a clean, professional, and aesthetic look\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "    font-family: 'Arial', sans-serif;\n",
        "}\n",
        ".gr-button {\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px 24px !important;\n",
        "    font-weight: 600 !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    opacity: 0.9 !important;\n",
        "    transform: scale(1.02) !important;\n",
        "}\n",
        ".gr-textbox {\n",
        "    border-radius: 12px !important;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1) !important;\n",
        "    padding: 15px !important;\n",
        "}\n",
        ".card {\n",
        "    background: #ffffff !important;\n",
        "    border-radius: 16px !important;\n",
        "    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.15) !important;\n",
        "    padding: 20px !important;\n",
        "    margin-bottom: 20px !important;\n",
        "}\n",
        ".header {\n",
        "    background: linear-gradient(135deg, #3b82f6, #60a5fa) !important;\n",
        "    color: white !important;\n",
        "    padding: 20px !important;\n",
        "    border-radius: 12px !important;\n",
        "    text-align: center !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Build the Gradio interface with a modern design\n",
        "with gr.Blocks(theme=gr.themes.Monochrome(), css=custom_css) as interface:\n",
        "    # Add a stylish header to welcome users\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div class=\"header\">\n",
        "            <h1>ğŸ“ Generative Text with LSTM</h1>\n",
        "            <p>Generate creative text using a trained LSTM model. Provide a starting string and watch the AI write!</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # Input section for starting string\n",
        "        with gr.Column(scale=3):\n",
        "            with gr.Group(elem_classes=\"card\"):\n",
        "                gr.Markdown(\"### âœï¸ Input Settings\")\n",
        "                start_str = gr.Textbox(\n",
        "                    label=\"Starting String\",\n",
        "                    placeholder=\"Enter a starting string (e.g., 'Artificial')\",\n",
        "                    value=\"Artificial\"\n",
        "                )\n",
        "\n",
        "        # Output section for generated text\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Group(elem_classes=\"card\"):\n",
        "                gr.Markdown(\"### ğŸ“œ Generated Text\")\n",
        "                generated_text = gr.Textbox(\n",
        "                    label=\"Generated Text\",\n",
        "                    placeholder=\"Generated text will appear here...\",\n",
        "                    lines=10,\n",
        "                    max_lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"Status\",\n",
        "                    placeholder=\"Status messages will appear here...\",\n",
        "                    interactive=False,\n",
        "                    visible=True\n",
        "                )\n",
        "\n",
        "    # Buttons for actions\n",
        "    with gr.Row():\n",
        "        train_button = gr.Button(\"Train Model\", variant=\"secondary\")\n",
        "        generate_button = gr.Button(\"Generate Text\", variant=\"primary\")\n",
        "        clear_button = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "\n",
        "    # Connect buttons to functions\n",
        "    train_button.click(\n",
        "        fn=train_model,\n",
        "        inputs=[],\n",
        "        outputs=[status_output]\n",
        "    )\n",
        "    generate_button.click(\n",
        "        fn=generate_text,\n",
        "        inputs=[start_str],\n",
        "        outputs=[generated_text, status_output]\n",
        "    )\n",
        "    clear_button.click(\n",
        "        fn=lambda: (None, None),\n",
        "        inputs=None,\n",
        "        outputs=[generated_text, status_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "H_Gk1nrH2-R8",
        "outputId": "afdad60f-811f-47c6-f5c8-9a6ee9dc64c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://52bea8b8ccef8c5ecb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://52bea8b8ccef8c5ecb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}